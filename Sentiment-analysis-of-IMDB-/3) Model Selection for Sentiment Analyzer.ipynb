{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86584429",
   "metadata": {},
   "source": [
    "## Importing all dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6d02b",
   "metadata": {},
   "source": [
    "## Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21565e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_csv('IMDB-Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7142c7",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize stopword as per data\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"would\",\"shall\",\"could\",\"might\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")\n",
    "stop_words=set(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------Data Cleaning and Preprocessing pipeline----------------------------------'''\n",
    "\n",
    "#Removing special character\n",
    "def remove_special_character(content):\n",
    "    return re.sub('\\W+',' ', content )#re.sub('\\[[^&@#!]]*\\]', '', content)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "# Expansion of english contractions\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"weren\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    '''content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)'''\n",
    "    return content\n",
    "\n",
    "#Data preprocessing\n",
    "def data_cleaning(content):\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    \n",
    "    content = remove_stopwords(content)    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "#Data cleaning\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping rating data to Binary label 1 (+ve) if rating >=7 and 0 (-ve) if rating <=4 and 2 (neutral) if rating = 5 or 6\n",
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "#Removing \n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Reviews','Ratings','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad126e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies for feature engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755bcab",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bde6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of word \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ad57b",
   "metadata": {},
   "source": [
    "## Vectorization with Count Vectorizer and TDIDF Vectorizer with unigram, bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d38c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "#countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)\n",
    "#x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "#x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc64438",
   "metadata": {},
   "source": [
    "## Feature Selection with Chi squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 5000\n",
    "Number = 1\n",
    "featureselection = PrettyTable([\"Unigram\", \"Bigram\",\"Trigram\"])\n",
    "for category in train['Label'].unique():\n",
    "    features_chi2 = chi2(x_train_tfidf, train['Label'] == category)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidfvect.get_feature_names_out())[indices]\n",
    "\n",
    "    unigrams = [x for x in feature_names if len(x.split(' ')) == 1]\n",
    "    bigrams = [x for x in feature_names if len(x.split(' ')) == 2]\n",
    "    trigrams = [x for x in feature_names if len(x.split(' ')) == 3]\n",
    "   \n",
    "    Number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633921c3",
   "metadata": {},
   "source": [
    "## Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ed43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prerequisite libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b534b",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_1=LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977f08d",
   "metadata": {},
   "source": [
    "## Training of Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_1.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f84562",
   "metadata": {},
   "source": [
    "## Evaluation on Test and Train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdfb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Logistic Regression: %s\" % precision_score(y_train,model_1.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Logistic Regression: %s\" % roc_auc_score(y_train,model_1.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_1 =f1_score(y_train,model_1.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score ftraining dateset for Logistic Regression: %s\" % f1_score_train_1)\n",
    "print(\"Precision Score on test for Logistic Regression: %s\" % precision_score(y_test,model_1.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Logistic Regression: %s\" % roc_auc_score(y_test,model_1.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_1 =f1_score(y_test,model_1.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Logistic Regression: %s\" % f1_score_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7b847",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Pipeline(\n",
    "    steps=[\n",
    "        #(\"classifier\", DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)),\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701dafb",
   "metadata": {},
   "source": [
    "## Training of Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae23f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_2.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195d16f",
   "metadata": {},
   "source": [
    "## Evaluation on test data and training data of Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Decision Tree Classifier: %s\" % precision_score(y_train,model_2.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Decision Tree Classifier: %s\" % roc_auc_score(y_train,model_2.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_2 =f1_score(y_train,model_2.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Decision Tree Classifier: %s\" % f1_score_train_2)\n",
    "print(\"Precision Score on test for Decision Tree Classifier: %s\" % precision_score(y_test,model_2.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Decision Tree Classifier: %s\" % roc_auc_score(y_test,model_2.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_2 =f1_score(y_test,model_2.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Decision Tree Classifier: %s\" % f1_score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4eeb0",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier with max depth 11 to fix overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ebf867",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Pipeline(\n",
    "    steps=[\n",
    "        (\"classifier\", DecisionTreeClassifier( criterion='gini', max_depth=11, min_samples_split=2, min_samples_leaf=1)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9758b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_3.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5368e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Decision Tree Classifier: %s\" % precision_score(y_train,model_3.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Decision Tree Classifier: %s\" % roc_auc_score(y_train,model_3.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_3 =f1_score(y_train,model_3.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Decision Tree Classifier: %s\" % f1_score_train_3)\n",
    "print(\"Precision Score on test for Decision Tree Classifier: %s\" % precision_score(y_test,model_3.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Decision Tree Classifier: %s\" % roc_auc_score(y_test,model_3.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_3 =f1_score(y_test,model_3.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Decision Tree Classifier: %s\" % f1_score_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f5f8e6",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Pipeline(\n",
    "    steps=[\n",
    "        #(\"classifier\", RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=15, min_samples_split=3, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None))\n",
    "    (\"classifier\", RandomForestClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f237d9",
   "metadata": {},
   "source": [
    "## Training of Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0883f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_4.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e83a90",
   "metadata": {},
   "source": [
    "## Evaluation on test data and training data of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d25be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Random Forest Classifier: %s\" % precision_score(y_train,model_4.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Random Forest Classifier: %s\" % roc_auc_score(y_train,model_4.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_4 =f1_score(y_train,model_4.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Random Forest Classifier: %s\" % f1_score_train_4)\n",
    "print(\"Precision Score on test for Random Forest Classifier: %s\" % precision_score(y_test,model_4.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Random Forest Classifier: %s\" % roc_auc_score(y_test,model_4.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_4 =f1_score(y_test,model_4.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Random Forest Classifier: %s\" % f1_score_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b07a5",
   "metadata": {},
   "source": [
    "## Ada Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4365c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Pipeline(\n",
    "    steps=[\n",
    "        (\"classifier\", AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=4),\n",
    "    n_estimators=100,\n",
    "    learning_rate=.8)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc19e541",
   "metadata": {},
   "source": [
    "## Training of Ada boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d737c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_5.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8295fa1",
   "metadata": {},
   "source": [
    "## Evaluation on test data and training data of Ada boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f5aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Ada Boost Classifier: %s\" % precision_score(y_train,model_5.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Ada Boost Classifier: %s\" % roc_auc_score(y_train,model_5.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_5 =f1_score(y_train,model_5.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Ada Boost Classifier: %s\" % f1_score_train_5)\n",
    "print(\"Precision Score on test for Ada Boost Classifier: %s\" % precision_score(y_test,model_5.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Ada Boost Classifier: %s\" % roc_auc_score(y_test,model_5.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_5 =f1_score(y_test,model_5.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Random Forest Classifier: %s\" % f1_score_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e9834",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def hyperparamtune(classifier, param_grid,metric,verbose_value,cv):\n",
    "    model=model_selection.GridSearchCV(\n",
    "            estimator=classifier,\n",
    "            param_grid=param_grid,\n",
    "            scoring=metric,\n",
    "            verbose=verbose_value,            \n",
    "            cv=cv)\n",
    "\n",
    "    model.fit(x_train_tfidf,y_train)\n",
    "    print(\"Best Score %s\" % {model.best_score_})\n",
    "    print(\"Best hyperparameter set:\")\n",
    "    best_parameters = model.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(f\"\\t{param_name}: {best_parameters[param_name]}\")\n",
    "    return model, best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebbdb1",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc668972",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_gd={\"penalty\":[\"l2\",\"l1\"],\n",
    "         \"C\":[0.01,0.1,1.0,10],\n",
    "         \"tol\":[0.0001,0.001,0.01],\n",
    "         \"max_iter\":[100,200]}\n",
    "model_7, best_param = hyperparamtune(LogisticRegression(),param_gd,\"accuracy\",10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc7100",
   "metadata": {},
   "source": [
    "## Evaluation of FineTuned Logsitic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Finetuned Logsitic Regression Classifier: %s\" % precision_score(y_train,model_7.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Finetuned Logsitic Regression Classifier: %s\" % roc_auc_score(y_train,model_7.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_7 =f1_score(y_train,model_7.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Finetuned Logsitic Regression Classifier: %s\" % f1_score_train_7)\n",
    "print(\"Precision Score on test for Finetuned Logsitic Regression Classifier: %s\" % precision_score(y_test,model_7.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Finetuned Logsitic Regression Classifier: %s\" % roc_auc_score(y_test,model_7.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_7 =f1_score(y_test,model_7.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Finetuned Logsitic Regression Classifier: %s\" % f1_score_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef696",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Define grid of hyper parameters\n",
    "param_gd={\"n_estimators\":[100,200,300],\n",
    "         \"max_depth\":[11,13,17,19,23],\n",
    "         \"criterion\":[\"gini\",\"entropy\"],\n",
    "         \"min_samples_split\":[3,7,11],\n",
    "         \"min_samples_leaf\":[3,5],\n",
    "         \"max_features\":[\"sqrt\", \"log2\"]}\n",
    "\n",
    "model_8, best_param_8 = hyperparamtune(RandomForestClassifier(),param_gd,\"accuracy\",10,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69344011",
   "metadata": {},
   "source": [
    "## Evaluation of Finetuned Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96febed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score on training dateset for Finetuned Random Forest Classifier: %s\" % precision_score(y_train,model_8.predict(x_train_tfidf),average='micro'))\n",
    "print(\"AUC Score on training dateset for Finetuned Random Forest Classifier: %s\" % roc_auc_score(y_train,model_8.predict_proba(x_train_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_train_8 =f1_score(y_train,model_8.predict(x_train_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score training dateset for Finetuned Random Forest Classifier: %s\" % f1_score_train_8)\n",
    "print(\"Precision Score on test for Finetuned Random Forest Classifier: %s\" % precision_score(y_test,model_8.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score on test for Finetuned Random Forest Classifier: %s\" % roc_auc_score(y_test,model_8.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_8 =f1_score(y_test,model_8.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Finetuned Random Forest Classifier: %s\" % f1_score_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "model = None\n",
    "if ((f1_score_1>f1_score_2) & (f1_score_1>f1_score_3) & (f1_score_1>f1_score_4) & (f1_score_1>f1_score_5)& (f1_score_1>f1_score_7)&(f1_score_1>f1_score_8)):\n",
    "    model = model_1\n",
    "    print(\"Logsitics Regression is providing best F1 score: %f\" % f1_score_1)\n",
    "elif((f1_score_2>f1_score_1) & (f1_score_2>f1_score_3) & (f1_score_2>f1_score_4) & (f1_score_2>f1_score_5)& (f1_score_2>f1_score_7)&(f1_score_2>f1_score_8)):\n",
    "    model = model_2\n",
    "    print(\" Over fit Decision Tree is providing best F1 score: %f\" % f1_score_2)\n",
    "elif((f1_score_3>f1_score_1) & (f1_score_3>f1_score_2) & (f1_score_3>f1_score_4)&(f1_score_3>f1_score_5)& (f1_score_3>f1_score_7)&(f1_score_3>f1_score_8)):\n",
    "    model = model_3\n",
    "    print(\"Decision Tree is providing best F1 score: %f\" % f1_score_3)\n",
    "elif((f1_score_4>f1_score_1) & (f1_score_4>f1_score_2) & (f1_score_4>f1_score_3)&(f1_score_4>f1_score_5)& (f1_score_4>f1_score_7)&(f1_score_4>f1_score_8)):\n",
    "    model = model_4\n",
    "    print(\"Random Forest is providing best F1 score: %f\" % f1_score_4)\n",
    "elif((f1_score_5>f1_score_1) & (f1_score_5>f1_score_2) & (f1_score_5>f1_score_4)&(f1_score_5>f1_score_3)& (f1_score_5>f1_score_7)&(f1_score_5>f1_score_8)):\n",
    "    model = model_5\n",
    "    print(\"Adaboost Classifier is providing best F1 score: %f\" % f1_score_5)\n",
    "elif((f1_score_7>f1_score_1) & (f1_score_7>f1_score_2) & (f1_score_7>f1_score_4)&(f1_score_7>f1_score_3)& (f1_score_7>f1_score_5)&(f1_score_7>f1_score_8)):\n",
    "    model = model_7\n",
    "    print(\"Finetuned Logsitics Regression Classifier is providing best F1 score: %f\" % f1_score_7)\n",
    "elif((f1_score_8>f1_score_1) & (f1_score_8>f1_score_2) & (f1_score_8>f1_score_4)&(f1_score_8>f1_score_3)& (f1_score_8>f1_score_7)&(f1_score_8>f1_score_5)):\n",
    "    model = model_8\n",
    "    print(\"Finetuned Random Forest Classifier is providing best F1 score: %f\" % f1_score_8)\n",
    "else:\n",
    "    print(\"No Model is selected, Train again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145fdb55-a821-4e91-a0fe-2d0e5d9d8834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014eaa1-9404-4e12-947a-32d5f51f593a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0fc93b-d87f-474c-b58b-73c2b067e546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dcb48-3ab4-44b8-b3ec-029db4c52d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175513b-ecac-47a2-a555-d4d314eb68fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd04db-054c-4146-948b-7281b10b4e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41371ad8-e775-4683-8da3-8a66d1e12ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4ec81-b041-46fb-9768-5f23d46bfaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571dfdf8-db66-4d24-8bb3-b2e5f5fd8b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18ded2-c616-4cf5-a4df-3272e6a337ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc471ec9-35bf-43b4-ac44-89e0f7c2f32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c0386-34ce-479a-8199-2ff4d7915a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37347785-ad29-43e5-9db6-3c469070b9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bb0e0-5ff5-413d-b70c-1a6adccdb5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1223e-f4d1-4426-8a1c-3d793fc8ab4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03dbaf-fbd2-418c-ad35-45ee4dd8751c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61cd983-d14a-44ec-9dbf-e614713f9b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4290fe0-a7a1-419e-a465-21a00bbca218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
